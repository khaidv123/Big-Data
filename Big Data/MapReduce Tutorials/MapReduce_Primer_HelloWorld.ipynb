{"cells":[{"cell_type":"markdown","metadata":{"id":"GzbmlR27wh6e"},"source":["<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n","\n","# MapReduce: A Primer with <code>Hello World!</code>\n","<br>\n","<br>\n","\n","For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n","\n","> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n","\n","(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n","\n","We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n","\n","> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n","\n","MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n","\n","Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"]},{"cell_type":"markdown","metadata":{"id":"uUbM5R0GwwYw"},"source":["# Download core Hadoop"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDgQtQlzw8bL","outputId":"8a5ba206-8265-41e7-d3fa-2007171680bf","executionInfo":{"status":"ok","timestamp":1715009590332,"user_tz":-420,"elapsed":61054,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}],"source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"]},{"cell_type":"markdown","metadata":{"id":"3yvb5cw9xEbh"},"source":["# Set environment variables"]},{"cell_type":"markdown","metadata":{"id":"u6lkrz1dxIiO"},"source":["## Set `HADOOP_HOME` and `PATH`"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7maAwaFxBT_","outputId":"a520447c-8a6d-4c74-95e9-2233586094d9","executionInfo":{"status":"ok","timestamp":1715009590333,"user_tz":-420,"elapsed":11,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}],"source":["# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"]},{"cell_type":"markdown","metadata":{"id":"4kzJ8cNoxPyK"},"source":["## Set `JAVA_HOME`\n","\n","While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SauFHVPOxL-Y","outputId":"8249b4f8-1018-40d0-d3eb-5d908b4e98b9","executionInfo":{"status":"ok","timestamp":1715009590333,"user_tz":-420,"elapsed":9,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}],"source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"]},{"cell_type":"markdown","metadata":{"id":"6HFPVX84xbNd"},"source":["# Run a MapReduce job with Hadoop streaming"]},{"cell_type":"markdown","metadata":{"id":"_yVa55X1xmOb"},"source":["## Create a file\n","\n","Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9Jz7mJkcxYxw","executionInfo":{"status":"ok","timestamp":1715009590333,"user_tz":-420,"elapsed":6,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[],"source":["!echo \"Hello, World!\">./hello.txt"]},{"cell_type":"markdown","metadata":{"id":"zSh_Kr5Bxvst"},"source":["## Launch the MapReduce \"Hello, World!\" application\n","\n","Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n","\n","Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n","\n","**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb5JryK9xpPA","outputId":"9a018bcf-7aa7-40d5-a371-c497bcac8b7b","executionInfo":{"status":"ok","timestamp":1715009597997,"user_tz":-420,"elapsed":7668,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["rm: `my_output': No such file or directory\n","2024-05-06 15:33:14,186 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:33:14,467 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:33:14,468 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:33:14,507 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:33:14,861 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:33:14,898 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:33:15,441 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2049410609_0001\n","2024-05-06 15:33:15,442 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:33:15,688 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:33:15,691 INFO mapreduce.Job: Running job: job_local2049410609_0001\n","2024-05-06 15:33:15,701 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:33:15,704 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:33:15,712 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:15,712 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:15,783 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:33:15,789 INFO mapred.LocalJobRunner: Starting task: attempt_local2049410609_0001_m_000000_0\n","2024-05-06 15:33:15,826 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:15,829 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:15,862 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:33:15,874 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-05-06 15:33:15,892 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:33:15,967 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:33:15,968 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:33:15,968 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:33:15,968 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:33:15,968 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:33:16,009 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:33:16,020 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-05-06 15:33:16,054 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:33:16,056 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:33:16,056 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:33:16,063 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:33:16,064 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:33:16,065 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:33:16,077 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:33:16,078 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:33:16,078 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:33:16,079 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:33:16,080 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:33:16,081 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:33:16,100 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:33:16,102 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-05-06 15:33:16,105 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:33:16,105 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:33:16,109 INFO mapred.LocalJobRunner: \n","2024-05-06 15:33:16,109 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:33:16,109 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:33:16,109 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n","2024-05-06 15:33:16,109 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-05-06 15:33:16,121 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:33:16,141 INFO mapred.Task: Task:attempt_local2049410609_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:33:16,147 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-05-06 15:33:16,148 INFO mapred.Task: Task 'attempt_local2049410609_0001_m_000000_0' done.\n","2024-05-06 15:33:16,161 INFO mapred.Task: Final Counters for attempt_local2049410609_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=857635\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=407896064\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-05-06 15:33:16,161 INFO mapred.LocalJobRunner: Finishing task: attempt_local2049410609_0001_m_000000_0\n","2024-05-06 15:33:16,179 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:33:16,183 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:33:16,184 INFO mapred.LocalJobRunner: Starting task: attempt_local2049410609_0001_r_000000_0\n","2024-05-06 15:33:16,199 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:16,200 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:16,200 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:33:16,205 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@57cbb3db\n","2024-05-06 15:33:16,207 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:33:16,237 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:33:16,260 INFO reduce.EventFetcher: attempt_local2049410609_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:33:16,361 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2049410609_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n","2024-05-06 15:33:16,378 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local2049410609_0001_m_000000_0\n","2024-05-06 15:33:16,387 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n","2024-05-06 15:33:16,396 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:33:16,398 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:33:16,398 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:33:16,409 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:33:16,409 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-05-06 15:33:16,411 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:33:16,412 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n","2024-05-06 15:33:16,413 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:33:16,413 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:33:16,414 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-05-06 15:33:16,415 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:33:16,424 INFO mapred.Task: Task:attempt_local2049410609_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:33:16,426 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:33:16,426 INFO mapred.Task: Task attempt_local2049410609_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:33:16,428 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2049410609_0001_r_000000_0' to file:/content/my_output\n","2024-05-06 15:33:16,429 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-05-06 15:33:16,429 INFO mapred.Task: Task 'attempt_local2049410609_0001_r_000000_0' done.\n","2024-05-06 15:33:16,430 INFO mapred.Task: Final Counters for attempt_local2049410609_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141992\n","\t\tFILE: Number of bytes written=857685\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=407896064\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-05-06 15:33:16,430 INFO mapred.LocalJobRunner: Finishing task: attempt_local2049410609_0001_r_000000_0\n","2024-05-06 15:33:16,430 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:33:16,698 INFO mapreduce.Job: Job job_local2049410609_0001 running in uber mode : false\n","2024-05-06 15:33:16,699 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:33:16,700 INFO mapreduce.Job: Job job_local2049410609_0001 completed successfully\n","2024-05-06 15:33:16,743 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283906\n","\t\tFILE: Number of bytes written=1715320\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=815792128\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-05-06 15:33:16,743 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"]},{"cell_type":"markdown","metadata":{"id":"OB_fX9u5x55y"},"source":["## Verify the result\n","\n","If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n","\n","Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnvEvYDfx2g4","outputId":"6b0cd434-c9c1-472b-9751-176c14924725","executionInfo":{"status":"ok","timestamp":1715009601887,"user_tz":-420,"elapsed":3895,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}],"source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"]},{"cell_type":"markdown","metadata":{"id":"BLMnBh44x_YR"},"source":["**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufAfmGUvx8jW","outputId":"e8aac365-0d15-4c71-8b03-99811f8c6854","executionInfo":{"status":"ok","timestamp":1715009604144,"user_tz":-420,"elapsed":2261,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-05-06 15:33 my_output/_SUCCESS\n","-rw-r--r--   1 root root         15 2024-05-06 15:33 my_output/part-00000\n"]}],"source":["!hdfs dfs -ls my_output"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnKSahPzyCAn","outputId":"adc1a19c-e454-4c77-9f99-207d1cef1fef","executionInfo":{"status":"ok","timestamp":1715009604145,"user_tz":-420,"elapsed":11,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total 4\n","-rw-r--r-- 1 root root 15 May  6 15:33 part-00000\n","-rw-r--r-- 1 root root  0 May  6 15:33 _SUCCESS\n"]}],"source":["!ls -l my_output"]},{"cell_type":"markdown","metadata":{"id":"v9LmpcaMyG23"},"source":["The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eL-Clat5yD8I","outputId":"78bf51f9-f7c5-4f61-d5b1-cc75088cbc88","executionInfo":{"status":"ok","timestamp":1715009604145,"user_tz":-420,"elapsed":7,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, World!\t\n"]}],"source":["!cat my_output/part-00000"]},{"cell_type":"markdown","metadata":{"id":"AmpHr_HyyMnM"},"source":["# MapReduce without specifying mapper or reducer\n","\n","In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n","\n","Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPWL1AiXyJac","outputId":"27cb054e-9205-4471-cb0a-8dc22845b48d","executionInfo":{"status":"ok","timestamp":1715009605377,"user_tz":-420,"elapsed":1236,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-06 15:33:24,250 ERROR streaming.StreamJob: Unrecognized option: -h\n","Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n","\n","Try -help for more information\n","Streaming Command Failed!\n"]}],"source":["!mapred streaming -h"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H2MkIUPyQc2","outputId":"0571823e-2f79-4eac-bd05-0cee2833ae6e","executionInfo":{"status":"ok","timestamp":1715009612763,"user_tz":-420,"elapsed":7390,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:33:26,516 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-05-06 15:33:28,917 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:33:29,114 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:33:29,114 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:33:29,137 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:33:29,449 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:33:29,477 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:33:29,808 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local30771770_0001\n","2024-05-06 15:33:29,809 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:33:30,061 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:33:30,063 INFO mapreduce.Job: Running job: job_local30771770_0001\n","2024-05-06 15:33:30,072 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:33:30,076 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:33:30,090 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:30,091 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:30,180 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:33:30,186 INFO mapred.LocalJobRunner: Starting task: attempt_local30771770_0001_m_000000_0\n","2024-05-06 15:33:30,234 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:30,237 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:30,271 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:33:30,283 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-05-06 15:33:30,301 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:33:30,395 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:33:30,395 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:33:30,395 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:33:30,395 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:33:30,395 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:33:30,404 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:33:30,413 INFO mapred.LocalJobRunner: \n","2024-05-06 15:33:30,413 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:33:30,413 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:33:30,413 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n","2024-05-06 15:33:30,414 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-05-06 15:33:30,428 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:33:30,452 INFO mapred.Task: Task:attempt_local30771770_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:33:30,457 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-05-06 15:33:30,457 INFO mapred.Task: Task 'attempt_local30771770_0001_m_000000_0' done.\n","2024-05-06 15:33:30,468 INFO mapred.Task: Final Counters for attempt_local30771770_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=848641\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=390070272\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-05-06 15:33:30,469 INFO mapred.LocalJobRunner: Finishing task: attempt_local30771770_0001_m_000000_0\n","2024-05-06 15:33:30,470 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:33:30,474 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:33:30,479 INFO mapred.LocalJobRunner: Starting task: attempt_local30771770_0001_r_000000_0\n","2024-05-06 15:33:30,492 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:30,492 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:30,493 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:33:30,500 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@12d32682\n","2024-05-06 15:33:30,503 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:33:30,531 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:33:30,537 INFO reduce.EventFetcher: attempt_local30771770_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:33:30,619 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local30771770_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n","2024-05-06 15:33:30,627 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local30771770_0001_m_000000_0\n","2024-05-06 15:33:30,632 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n","2024-05-06 15:33:30,637 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:33:30,639 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:33:30,639 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:33:30,656 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:33:30,657 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-05-06 15:33:30,659 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:33:30,663 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n","2024-05-06 15:33:30,666 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:33:30,666 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:33:30,676 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-05-06 15:33:30,677 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:33:30,692 INFO mapred.Task: Task:attempt_local30771770_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:33:30,704 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:33:30,705 INFO mapred.Task: Task attempt_local30771770_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:33:30,708 INFO output.FileOutputCommitter: Saved output of task 'attempt_local30771770_0001_r_000000_0' to file:/content/my_output\n","2024-05-06 15:33:30,710 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-05-06 15:33:30,710 INFO mapred.Task: Task 'attempt_local30771770_0001_r_000000_0' done.\n","2024-05-06 15:33:30,712 INFO mapred.Task: Final Counters for attempt_local30771770_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142006\n","\t\tFILE: Number of bytes written=848699\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=16\n","\t\tTotal committed heap usage (bytes)=390070272\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-05-06 15:33:30,712 INFO mapred.LocalJobRunner: Finishing task: attempt_local30771770_0001_r_000000_0\n","2024-05-06 15:33:30,712 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:33:31,069 INFO mapreduce.Job: Job job_local30771770_0001 running in uber mode : false\n","2024-05-06 15:33:31,071 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:33:31,072 INFO mapreduce.Job: Job job_local30771770_0001 completed successfully\n","2024-05-06 15:33:31,096 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283920\n","\t\tFILE: Number of bytes written=1697340\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=16\n","\t\tTotal committed heap usage (bytes)=780140544\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-05-06 15:33:31,100 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output"]},{"cell_type":"markdown","metadata":{"id":"v7Ks3e96yXuB"},"source":["## Verify the result"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWAXvG0_yThc","outputId":"8305163d-6d2a-461b-d1ac-7157bd9b173c","executionInfo":{"status":"ok","timestamp":1715009614981,"user_tz":-420,"elapsed":2231,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}],"source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"]},{"cell_type":"markdown","metadata":{"id":"t40GgJ2Hya9P"},"source":["Show output"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5APWEgoyaRS","outputId":"680e64e6-f895-482f-9e67-5d5dfef64a95","executionInfo":{"status":"ok","timestamp":1715009615344,"user_tz":-420,"elapsed":366,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0\tHello, World!\n"]}],"source":["!cat my_output/part-00000"]},{"cell_type":"markdown","metadata":{"id":"mzfaMVKqyjpC"},"source":["What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."]},{"cell_type":"markdown","metadata":{"id":"lzIuWv7Myndc"},"source":["# Run a map-only MapReduce job\n","\n","Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n","\n","To run a MapReduce job _without_ reducer one needs to use the generic option\n","\n","    \\-D mapreduce.job.reduces=0\n","\n","(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdwKWyVRye27","outputId":"a9e27af3-acb9-465c-f2f7-bcb060448f68","executionInfo":{"status":"ok","timestamp":1715009622302,"user_tz":-420,"elapsed":6962,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:33:36,101 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-05-06 15:33:38,589 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:33:38,765 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:33:38,765 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:33:38,788 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:33:39,062 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:33:39,088 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:33:39,439 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1526336130_0001\n","2024-05-06 15:33:39,439 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:33:39,704 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:33:39,707 INFO mapreduce.Job: Running job: job_local1526336130_0001\n","2024-05-06 15:33:39,716 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:33:39,719 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:33:39,727 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:39,727 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:39,799 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:33:39,805 INFO mapred.LocalJobRunner: Starting task: attempt_local1526336130_0001_m_000000_0\n","2024-05-06 15:33:39,849 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:39,850 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:39,893 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:33:39,909 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-05-06 15:33:39,935 INFO mapred.MapTask: numReduceTasks: 0\n","2024-05-06 15:33:39,964 INFO mapred.LocalJobRunner: \n","2024-05-06 15:33:39,977 INFO mapred.Task: Task:attempt_local1526336130_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:33:39,979 INFO mapred.LocalJobRunner: \n","2024-05-06 15:33:39,979 INFO mapred.Task: Task attempt_local1526336130_0001_m_000000_0 is allowed to commit now\n","2024-05-06 15:33:39,992 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1526336130_0001_m_000000_0' to file:/content/my_output\n","2024-05-06 15:33:39,993 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-05-06 15:33:39,993 INFO mapred.Task: Task 'attempt_local1526336130_0001_m_000000_0' done.\n","2024-05-06 15:33:40,005 INFO mapred.Task: Final Counters for attempt_local1526336130_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855489\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-05-06 15:33:40,005 INFO mapred.LocalJobRunner: Finishing task: attempt_local1526336130_0001_m_000000_0\n","2024-05-06 15:33:40,007 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:33:40,713 INFO mapreduce.Job: Job job_local1526336130_0001 running in uber mode : false\n","2024-05-06 15:33:40,716 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-05-06 15:33:40,721 INFO mapreduce.Job: Job job_local1526336130_0001 completed successfully\n","2024-05-06 15:33:40,727 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855489\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-05-06 15:33:40,728 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output"]},{"cell_type":"markdown","metadata":{"id":"QZIE9yXOyyHJ"},"source":["## Verify the result"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Dt3tUI0yu5e","outputId":"19cad8e0-f7ea-47e2-f647-e0d6e62672e1","executionInfo":{"status":"ok","timestamp":1715009624000,"user_tz":-420,"elapsed":1714,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0\tHello, World!\n"]}],"source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"]},{"cell_type":"markdown","metadata":{"id":"hUGEUv99y3cM"},"source":["## Why a map-only application?\n","\n","The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n","\n","On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"]},{"cell_type":"markdown","metadata":{"id":"FhVVFEdKzGcI"},"source":["# Improved version of the MapReduce \"Hello, World!\" application\n","\n","Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLgMXX2jy0vC","outputId":"d43ff127-85e4-4ee1-e842-758b065fc133","executionInfo":{"status":"ok","timestamp":1715009631454,"user_tz":-420,"elapsed":7457,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:33:45,419 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-05-06 15:33:48,161 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:33:48,347 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:33:48,347 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:33:48,374 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:33:48,658 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:33:48,692 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:33:49,044 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local533875228_0001\n","2024-05-06 15:33:49,045 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:33:49,315 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:33:49,317 INFO mapreduce.Job: Running job: job_local533875228_0001\n","2024-05-06 15:33:49,327 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:33:49,330 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:33:49,339 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:49,339 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:49,411 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:33:49,416 INFO mapred.LocalJobRunner: Starting task: attempt_local533875228_0001_m_000000_0\n","2024-05-06 15:33:49,454 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:33:49,457 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:33:49,486 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:33:49,506 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-05-06 15:33:49,528 INFO mapred.MapTask: numReduceTasks: 0\n","2024-05-06 15:33:49,554 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-05-06 15:33:49,567 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:33:49,570 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:33:49,571 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:33:49,571 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:33:49,572 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:33:49,572 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:33:49,573 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:33:49,574 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:33:49,574 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:33:49,575 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:33:49,577 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:33:49,578 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:33:49,602 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:33:49,609 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-05-06 15:33:49,609 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:33:49,610 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:33:49,613 INFO mapred.LocalJobRunner: \n","2024-05-06 15:33:49,625 INFO mapred.Task: Task:attempt_local533875228_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:33:49,627 INFO mapred.LocalJobRunner: \n","2024-05-06 15:33:49,627 INFO mapred.Task: Task attempt_local533875228_0001_m_000000_0 is allowed to commit now\n","2024-05-06 15:33:49,630 INFO output.FileOutputCommitter: Saved output of task 'attempt_local533875228_0001_m_000000_0' to file:/content/my_output\n","2024-05-06 15:33:49,631 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-05-06 15:33:49,631 INFO mapred.Task: Task 'attempt_local533875228_0001_m_000000_0' done.\n","2024-05-06 15:33:49,642 INFO mapred.Task: Final Counters for attempt_local533875228_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855001\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=365953024\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-05-06 15:33:49,643 INFO mapred.LocalJobRunner: Finishing task: attempt_local533875228_0001_m_000000_0\n","2024-05-06 15:33:49,644 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:33:50,324 INFO mapreduce.Job: Job job_local533875228_0001 running in uber mode : false\n","2024-05-06 15:33:50,327 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-05-06 15:33:50,330 INFO mapreduce.Job: Job job_local533875228_0001 completed successfully\n","2024-05-06 15:33:50,342 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855001\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=365953024\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-05-06 15:33:50,342 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sa1UDPr6zKKw","outputId":"210e21f3-10a7-4f07-f40b-2235860453a1","executionInfo":{"status":"ok","timestamp":1715009633930,"user_tz":-420,"elapsed":2480,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, World!\t\n"]}],"source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}