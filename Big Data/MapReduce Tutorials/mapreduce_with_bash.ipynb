{"cells":[{"cell_type":"markdown","metadata":{"id":"dUJFDC9DcZ9a"},"source":["# Mapreduce with bash\n","\n","In this notebook we're going to use `bash` to write a mapper and a reducer to count words in a file. This example will serve to illustrate the main features of Hadoop's MapReduce framework."]},{"cell_type":"code","source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50IA8hcQcwmb","outputId":"6f62610d-c5c1-4ddf-f4a3-eac93e346c2a","executionInfo":{"status":"ok","timestamp":1715009746623,"user_tz":-420,"elapsed":63312,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}]},{"cell_type":"code","source":["# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1r-WuKM3cyQq","outputId":"b51b1b5b-c0c8-4eb1-9a8b-2682ee0e249d","executionInfo":{"status":"ok","timestamp":1715009746623,"user_tz":-420,"elapsed":19,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}]},{"cell_type":"code","source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCpql1nDczjK","outputId":"be939f4a-250f-4492-81c3-f646ad17bf78","executionInfo":{"status":"ok","timestamp":1715009746624,"user_tz":-420,"elapsed":17,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}]},{"cell_type":"markdown","metadata":{"id":"KMuEAo7OcZ9c"},"source":["# Table of contents\n","- [What is MapReduce?](#mapreduce)\n","- [The mapper](#mapper)\n","    - [Test the mapper](#testmapper)\n","- [Hadoop it up](#hadoop)\n","    - [What is Hadoop Streaming?](#hadoopstreaming)\n","    - [List your Hadoop directory](#hdfs_ls)\n","    - [Test MapReduce with a dummy reducer](#dummyreducer)\n","    - [Shuffling and sorting](#shuffling&sorting)\n","- [The reducer](#reducer)\n","    - [Test and run](#run)\n","- [Run a mapreduce job with more data](#moredata)\n","    - [Sort the output with `sort`](#sortoutput)\n","    - [Sort the output with another MapReduce job](#sortoutputMR)\n","    - [Configure sort with `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n","    - [Specifying Configuration Variables with the -D Option](#configuration_variables)\n","    - [What is word count useful for?](#wordcount)\n"]},{"cell_type":"markdown","metadata":{"id":"gaoPwTjTcZ9c"},"source":["## What is MapReduce? <a name=\"mapreduce\"></a>\n","\n","MapReduce is a computing paradigm designed to allow parallel distributed processing of massive amounts of data.\n","\n","Data is split across several computer nodes, there it is processed by one or more mappers. The results emitted by the mappers are first sorted and then passed to one or more reducers that process and combine the data to return the final result.\n","\n","![Map & Reduce](mapreduce.png)\n","With [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) it is possible to use any programming language to define a mapper and/or a reducer. Here we're going to use the Unix `bash` scripting language ([here](https://www.gnu.org/software/bash/manual/html_node/index.html) is the official documentation for the language)."]},{"cell_type":"markdown","metadata":{"id":"mWigJEFgcZ9d"},"source":["## The mapper <a name=\"mapper\"></a>\n","Let's write a mapper script called `map.sh`. The mapper splits each input line into words and for each word it outputs a line containing the word and `1` separated by a tab.\n","\n","Example: for the input\n","<html>\n","<pre>\n","apple orange\n","banana apple peach\n","</pre>\n","</html>\n","\n","`map.sh` outputs:\n","<html>\n","<pre>\n","apple   1\n","orange  1\n","banana  1\n","apple  1\n","peach  1\n","</pre>\n","</html>\n","\n","\n","The <a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) allows us to write the contents of the cell to a file."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8JJYJ7OcZ9d","outputId":"472009c4-2893-497c-c631-8106c583f9a1","executionInfo":{"status":"ok","timestamp":1715009746624,"user_tz":-420,"elapsed":15,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing map.sh\n"]}],"source":["%%writefile map.sh\n","#!/bin/bash\n","\n","while read line\n","do\n"," for word in $line\n"," do\n","  if [ -n \"$word\" ]\n","  then\n","     echo -e ${word}\"\\t1\"\n","  fi\n"," done\n","done"]},{"cell_type":"markdown","metadata":{"id":"IUR_sbAgcZ9e"},"source":["After running the cell above, you should have a new file `map.sh` in your current directory.\n","The file can be seen in the left panel of JupyterLab or by using a list command on the bash command-line.\n","\n","**Note:** you can execute a single bash command in a Jupyter notebook cell by prepending an exclamation point to the command."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RUOcoz6cZ9e","outputId":"d7db11aa-5e79-4529-fc78-78b9266dd3ec","executionInfo":{"status":"ok","timestamp":1715009746624,"user_tz":-420,"elapsed":13,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r-- 1 root root 124 May  6 15:35 map.sh\n"]}],"source":["!ls -hl map.sh"]},{"cell_type":"markdown","metadata":{"id":"idZkMDG8cZ9e"},"source":["### Test the mapper <a name=\"testmapper\"></a>\n","We're going to test the mapper on on the command line with a small text file `fruits.txt` by first creating the text file.\n","In this file `apple` for instance appears two times, that's what we want our mapreduce job to compute."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuPO_tYrcZ9e","outputId":"b710faee-bff0-49d2-f461-6f1382f7a91d","executionInfo":{"status":"ok","timestamp":1715009746624,"user_tz":-420,"elapsed":9,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing fruits.txt\n"]}],"source":["%%writefile fruits.txt\n","apple banana\n","peach orange peach peach\n","pineapple peach apple"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNsSvhcvcZ9f","outputId":"e86b1f3a-f3bc-4f10-9dd1-1a057666af3f","executionInfo":{"status":"ok","timestamp":1715009747066,"user_tz":-420,"elapsed":448,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["apple banana\n","peach orange peach peach\n","pineapple peach apple\n"]}],"source":["!cat fruits.txt"]},{"cell_type":"markdown","metadata":{"id":"fib9TLhTcZ9f"},"source":["Test the mapper"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neItLQCOcZ9f","outputId":"8f8ff681-12fb-4cc3-c7ba-c8fb29f427d9","executionInfo":{"status":"ok","timestamp":1715009747066,"user_tz":-420,"elapsed":4,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: ./map.sh: Permission denied\n"]}],"source":["!cat fruits.txt|./map.sh"]},{"cell_type":"markdown","metadata":{"id":"0eHYmnE_cZ9g"},"source":["If the script `map.sh` does not have the executable bit set, you need to set the correct permissions."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Y2zoCkX4cZ9h","executionInfo":{"status":"ok","timestamp":1715009747067,"user_tz":-420,"elapsed":3,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[],"source":["!chmod 700 map.sh"]},{"cell_type":"markdown","metadata":{"id":"AXHNt8-qcZ9h"},"source":["## Hadoop it up <a name=\"hadoop\"></a>\n","Let us now run a MapReduce job with Hadoop Streaming."]},{"cell_type":"markdown","metadata":{"id":"hFfXiesIcZ9h"},"source":["### What is Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n","\n","Hadoop Streaming is a library included in the Hadoop distribution that enables you to develop MapReduce executables in languages other than Java.\n","\n","Mapper and/or reducer can be any sort of executables that read the input from stdin and emit the output to stdout. By default, input is read line by line and the prefix of a line up to the first tab character is the key; the rest of the line (excluding the tab character) will be the value.\n","\n","If there is no tab character in the line, then the entire line is considered as key and the value is null. The default input format is specified in the class `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) but this can can be customized for instance by defining another field separator (see the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n","\n","This is an example of MapReduce streaming invocation syntax:\n","<html>\n","<pre>\n","    mapred streaming \\\n","  -input myInputDirs \\\n","  -output myOutputDir \\\n","  -mapper /bin/cat \\\n","  -reducer /usr/bin/wc\n","\n","</pre>\n","</html>\n","\n","You can find the full official documentation for Hadoop Streaming from Apache Hadoop here: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n","\n","All options for the Hadoop Streaming command are described here: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options) and can be listed with the command"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7gMkPIqcZ9h","outputId":"63753a4f-7037-4e46-ccee-c6331c9de182","executionInfo":{"status":"ok","timestamp":1715009748295,"user_tz":-420,"elapsed":1231,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n"]}],"source":["!mapred streaming --help"]},{"cell_type":"markdown","metadata":{"id":"FMD5lJwccZ9h"},"source":["Now in order to run a mapreduce job that we need to \"upload\" the input file to the Hadoop file system."]},{"cell_type":"markdown","metadata":{"id":"vfHFqMcYcZ9h"},"source":["### List your Hadoop directory <a name=\"hdfs_ls\"></a>\n","\n","With the command `hdfs dfs -l` you can view the content of your HDFS home directory.\n","\n","`hdfs dfs` you can run a filesystem command on the Hadoop fileystem. The complete list of commands can be found in the [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5acSydbocZ9h","outputId":"d328086a-a62e-4e03-8c87-fb026a8ddb24","executionInfo":{"status":"ok","timestamp":1715009750164,"user_tz":-420,"elapsed":1871,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6 items\n","drwxr-xr-x   - root root       4096 2024-05-02 13:24 .config\n","-rw-r--r--   1 root root         60 2024-05-06 15:35 fruits.txt\n","drwxr-xr-x   - root root       4096 2024-03-04 08:05 hadoop-3.4.0\n","-rw-r--r--   1 root root  965537117 2024-05-06 15:35 hadoop-3.4.0.tar.gz\n","-rwx------   1 root root        124 2024-05-06 15:35 map.sh\n","drwxr-xr-x   - root root       4096 2024-05-02 13:25 sample_data\n"]}],"source":["!hdfs dfs -ls"]},{"cell_type":"markdown","metadata":{"id":"Sb-TIw76cZ9h"},"source":["Now create a directory `wordcount` with a subdirectory `input` on the Hadoop filesystem."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"K_Llq_WTcZ9h","executionInfo":{"status":"ok","timestamp":1715009752104,"user_tz":-420,"elapsed":1945,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[],"source":["%%bash\n","hdfs dfs -mkdir -p wordcount"]},{"cell_type":"markdown","metadata":{"id":"oXBDdolacZ9h"},"source":["Copy the file fruits.txt to Hadoop in the folder `wordcount/input`.\n","\n","Why do we need this step? Because the file `fruits.txt` needs to reside on the Hadoop filesystem in order to enjoy of all of the features of Hadoop (data partitioning, distributed processing, fault tolerance)."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"4SrweqylcZ9i","executionInfo":{"status":"ok","timestamp":1715009756439,"user_tz":-420,"elapsed":4339,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[],"source":["%%bash\n","hdfs dfs -rm -r wordcount/input 2>/dev/null\n","hdfs dfs -mkdir wordcount/input\n","hdfs dfs -put fruits.txt wordcount/input"]},{"cell_type":"markdown","metadata":{"id":"1w3bkhZrcZ9i"},"source":["Let's check if the file is there now.\n","\n","**Note:** it is convenient use the option `-h` for `ls` to show file sizes in human-readable form (showing sizes in Kilobytes, Megabytes, Gigabytes, etc.)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOUMhfT5cZ9i","outputId":"7e3b92fc-4c4c-4ff3-f594-81aadd3090de","executionInfo":{"status":"ok","timestamp":1715009757744,"user_tz":-420,"elapsed":1333,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r--   1 root root         60 2024-05-06 15:35 wordcount/input/fruits.txt\n"]}],"source":["!hdfs dfs -ls -h -R wordcount/input"]},{"cell_type":"markdown","metadata":{"id":"KUAV_0jbcZ9i"},"source":["### Test MapReduce with a dummy reducer <a name=\"dummyreducer\"></a>\n","\n","Let's try to run the mapper using a dummy reducer (`/bin/cat` does nothing else than echoing the data it receives).\n","\n","**Warning:** mapreduce tends to produce a verbose output, so be ready to see a long output. What you should look for is a message of the kind <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n","\n","**Note:** at the beginning of next cell you'll see a command `hadoop fs -rmr wordcount/output 2>/dev/null`. This is needed because when you run a job several times mapreduce will give an error if you try to overwrite the same output directory."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJ0ojz6RcZ9i","outputId":"e931f299-c58e-4b19-b3e1-d9dce51c25de","executionInfo":{"status":"ok","timestamp":1715009764002,"user_tz":-420,"elapsed":6261,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2024-05-06 15:35:59,849 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:36:00,117 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:36:00,117 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:36:00,135 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:00,404 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:36:00,444 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:36:00,826 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2144769453_0001\n","2024-05-06 15:36:00,826 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:36:01,333 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local2144769453_0001_3d69e4e0-9b30-487f-8916-a3157e723e41/map.sh\n","2024-05-06 15:36:01,468 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:36:01,469 INFO mapreduce.Job: Running job: job_local2144769453_0001\n","2024-05-06 15:36:01,473 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:36:01,481 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:36:01,488 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:01,488 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:01,568 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:36:01,579 INFO mapred.LocalJobRunner: Starting task: attempt_local2144769453_0001_m_000000_0\n","2024-05-06 15:36:01,646 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:01,646 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:01,679 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:01,696 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n","2024-05-06 15:36:01,723 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:36:01,778 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:36:01,778 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:36:01,778 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:36:01,778 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:36:01,778 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:36:01,781 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:36:01,788 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n","2024-05-06 15:36:01,800 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:36:01,802 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:36:01,808 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:36:01,808 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:36:01,809 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:36:01,809 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:36:01,810 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:36:01,810 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:36:01,810 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:36:01,811 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:36:01,811 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:36:01,811 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:36:01,853 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:01,856 INFO streaming.PipeMapRed: Records R/W=3/1\n","2024-05-06 15:36:01,861 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:01,861 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:01,864 INFO mapred.LocalJobRunner: \n","2024-05-06 15:36:01,864 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:36:01,864 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:36:01,864 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n","2024-05-06 15:36:01,864 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n","2024-05-06 15:36:01,886 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:36:01,912 INFO mapred.Task: Task:attempt_local2144769453_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:01,917 INFO mapred.LocalJobRunner: Records R/W=3/1\n","2024-05-06 15:36:01,917 INFO mapred.Task: Task 'attempt_local2144769453_0001_m_000000_0' done.\n","2024-05-06 15:36:01,924 INFO mapred.Task: Final Counters for attempt_local2144769453_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142117\n","\t\tFILE: Number of bytes written=861811\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tSpilled Records=9\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","2024-05-06 15:36:01,924 INFO mapred.LocalJobRunner: Finishing task: attempt_local2144769453_0001_m_000000_0\n","2024-05-06 15:36:01,924 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:36:01,929 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:36:01,932 INFO mapred.LocalJobRunner: Starting task: attempt_local2144769453_0001_r_000000_0\n","2024-05-06 15:36:01,953 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:01,953 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:01,953 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:01,962 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79160e44\n","2024-05-06 15:36:01,964 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:02,021 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:36:02,024 INFO reduce.EventFetcher: attempt_local2144769453_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:36:02,114 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2144769453_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n","2024-05-06 15:36:02,129 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local2144769453_0001_m_000000_0\n","2024-05-06 15:36:02,133 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n","2024-05-06 15:36:02,136 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:36:02,143 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:02,144 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:36:02,158 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:02,158 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-05-06 15:36:02,166 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:36:02,166 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n","2024-05-06 15:36:02,167 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:36:02,167 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:02,172 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-05-06 15:36:02,172 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:02,174 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-05-06 15:36:02,176 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-05-06 15:36:02,180 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-05-06 15:36:02,215 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:02,219 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:02,221 INFO streaming.PipeMapRed: Records R/W=9/1\n","2024-05-06 15:36:02,222 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:02,223 INFO mapred.Task: Task:attempt_local2144769453_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:02,224 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:02,224 INFO mapred.Task: Task attempt_local2144769453_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:36:02,228 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2144769453_0001_r_000000_0' to file:/content/wordcount/output\n","2024-05-06 15:36:02,235 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n","2024-05-06 15:36:02,235 INFO mapred.Task: Task 'attempt_local2144769453_0001_r_000000_0' done.\n","2024-05-06 15:36:02,236 INFO mapred.Task: Final Counters for attempt_local2144769453_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142353\n","\t\tFILE: Number of bytes written=862003\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=9\n","\t\tSpilled Records=9\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=21\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=90\n","2024-05-06 15:36:02,236 INFO mapred.LocalJobRunner: Finishing task: attempt_local2144769453_0001_r_000000_0\n","2024-05-06 15:36:02,236 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:36:02,473 INFO mapreduce.Job: Job job_local2144769453_0001 running in uber mode : false\n","2024-05-06 15:36:02,474 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:36:02,475 INFO mapreduce.Job: Job job_local2144769453_0001 completed successfully\n","2024-05-06 15:36:02,488 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=284470\n","\t\tFILE: Number of bytes written=1723814\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=9\n","\t\tSpilled Records=18\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=21\n","\t\tTotal committed heap usage (bytes)=864026624\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","\tFile Output Format Counters \n","\t\tBytes Written=90\n","2024-05-06 15:36:02,488 INFO streaming.StreamJob: Output directory: wordcount/output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/output 2>/dev/null\n","mapred streaming \\\n","  -files map.sh \\\n","  -input wordcount/input \\\n","  -output wordcount/output \\\n","  -mapper map.sh \\\n","  -reducer /bin/cat"]},{"cell_type":"markdown","metadata":{"id":"-OGI8ZA3cZ9i"},"source":["The output of the mapreduce job is in the `output` subfolder of the input directory. Let's check what's inside it."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXzmULQlcZ9i","outputId":"0fd4bb7e-1dd2-4b6c-d1be-740752d9160c","executionInfo":{"status":"ok","timestamp":1715009765129,"user_tz":-420,"elapsed":1151,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-05-06 15:36 wordcount/output/_SUCCESS\n","-rw-r--r--   1 root root         78 2024-05-06 15:36 wordcount/output/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output"]},{"cell_type":"markdown","metadata":{"id":"niDzBitMcZ9i"},"source":["If `output` contains a file named `_SUCCESS` that means that the mapreduce job completed successfully.\n","\n","**Note:** when dealing with Big Data it's always advisable to pipe the output of `cat` commands to `head` (or `tail`)."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbdT3i95cZ9i","outputId":"04361650-451d-4803-b40b-49c849e08efc","executionInfo":{"status":"ok","timestamp":1715009766519,"user_tz":-420,"elapsed":1394,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["apple\t1\n","apple\t1\n","banana\t1\n","orange\t1\n","peach\t1\n","peach\t1\n","peach\t1\n","peach\t1\n","pineapple\t1\n"]}],"source":["!hdfs dfs -cat wordcount/output/part*|head"]},{"cell_type":"markdown","metadata":{"id":"WStF2o6ncZ9j"},"source":["We have gotten as expected all the output from the mapper. Something worth of notice is that the data outputted from the mapper _**has been sorted**_. We haven't asked for that but this step is automatically performed by the mapper as soon as the number of reducers is $\\gt 0$."]},{"cell_type":"markdown","metadata":{"id":"J2vrcEs0cZ9j"},"source":["### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n","The following picture illustrates the concept of shuffling and sorting that is automatically performed by Hadoop after each map before passing the output to reduce. In the picture the outputs of the two mapper tasks are shown. The arrows represent shuffling and sorting done before delivering the data to one reducer (rightmost box).\n","![Shuffle & sort](shuffle_sort.png)\n","The shuffling and sorting phase is often one of the most costly in a MapReduce job.\n","\n","\n","<b>Note:</b> the job ran with two mappers because $2$ is the default number of mappers in Hadoop."]},{"cell_type":"markdown","metadata":{"id":"uo6XSxO_cZ9j"},"source":["## The reducer <a name=\"reducer\"></a>\n","Let's now write a reducer script called `reduce.sh`."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxAxFlRicZ9j","outputId":"32429ab2-9225-4937-d256-dac3579e6567","executionInfo":{"status":"ok","timestamp":1715009766867,"user_tz":-420,"elapsed":349,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing reduce.sh\n"]}],"source":["%%writefile reduce.sh\n","#!/bin/bash\n","\n","currkey=\"\"\n","currcount=0\n","while IFS=$'\\t' read -r key val\n","do\n","  if [[ $key == $currkey ]]\n","  then\n","      currcount=$(( currcount + val ))\n","  else\n","    if [ -n \"$currkey\" ]\n","    then\n","      echo -e ${currkey} \"\\t\" ${currcount}\n","    fi\n","    currkey=$key\n","    currcount=1\n","  fi\n","done\n","# last one\n","echo -e ${currkey} \"\\t\" ${currcount}"]},{"cell_type":"markdown","metadata":{"id":"W4LqAui6cZ9j"},"source":["Set permission for the reducer script."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"4tOLu-JvcZ9j","executionInfo":{"status":"ok","timestamp":1715009766867,"user_tz":-420,"elapsed":11,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[],"source":["!chmod 700 reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"qGHHFQpscZ9j"},"source":["### Test and run <a name=\"run\"></a>\n","\n","Test map and reduce on the shell"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp4GY33TcZ9k","outputId":"3f0d031a-f71e-46ee-98ea-3fe6ff21c319","executionInfo":{"status":"ok","timestamp":1715009766868,"user_tz":-420,"elapsed":10,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["apple \t 2\n","banana \t 1\n","orange \t 1\n","peach \t 4\n","pineapple \t 1\n"]}],"source":["!cat fruits.txt|./map.sh|sort|./reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"ea3IJtAvcZ9k"},"source":["Once we've made sure that the reducer script runs correctly on the shell, we can run it on the cluster."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eek_v0dWcZ9k","outputId":"e466dc10-8acf-4734-e385-bb4c24dd9dad","executionInfo":{"status":"ok","timestamp":1715009771904,"user_tz":-420,"elapsed":5041,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/output\n","packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob7943125111096304041.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:36:08,007 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-05-06 15:36:08,610 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:36:08,733 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:36:08,733 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:36:08,751 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:08,927 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:36:08,949 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:36:09,176 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local384800455_0001\n","2024-05-06 15:36:09,176 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:36:09,533 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local384800455_0001_b7690423-7f3f-4b86-94b5-903f26c0fffb/map.sh\n","2024-05-06 15:36:09,567 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local384800455_0001_85dc9bdc-3b40-466a-b289-e1a6a027bee5/reduce.sh\n","2024-05-06 15:36:09,647 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:36:09,648 INFO mapreduce.Job: Running job: job_local384800455_0001\n","2024-05-06 15:36:09,653 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:36:09,655 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:36:09,661 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:09,661 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:09,706 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:36:09,709 INFO mapred.LocalJobRunner: Starting task: attempt_local384800455_0001_m_000000_0\n","2024-05-06 15:36:09,745 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:09,753 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:09,786 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:09,797 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n","2024-05-06 15:36:09,822 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:36:09,862 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:36:09,862 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:36:09,862 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:36:09,862 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:36:09,862 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:36:09,865 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:36:09,872 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n","2024-05-06 15:36:09,877 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:36:09,885 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:36:09,886 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:36:09,886 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:36:09,887 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:36:09,887 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:36:09,888 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:36:09,889 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:36:09,889 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:36:09,889 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:36:09,890 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:36:09,890 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:36:09,920 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:09,921 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:09,923 INFO streaming.PipeMapRed: Records R/W=3/1\n","2024-05-06 15:36:09,923 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:09,925 INFO mapred.LocalJobRunner: \n","2024-05-06 15:36:09,925 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:36:09,925 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:36:09,926 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n","2024-05-06 15:36:09,926 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n","2024-05-06 15:36:09,933 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:36:09,947 INFO mapred.Task: Task:attempt_local384800455_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:09,951 INFO mapred.LocalJobRunner: Records R/W=3/1\n","2024-05-06 15:36:09,952 INFO mapred.Task: Task 'attempt_local384800455_0001_m_000000_0' done.\n","2024-05-06 15:36:09,974 INFO mapred.Task: Final Counters for attempt_local384800455_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1193\n","\t\tFILE: Number of bytes written=716554\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tSpilled Records=9\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=11\n","\t\tTotal committed heap usage (bytes)=313524224\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","2024-05-06 15:36:09,981 INFO mapred.LocalJobRunner: Finishing task: attempt_local384800455_0001_m_000000_0\n","2024-05-06 15:36:09,981 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:36:09,985 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:36:09,992 INFO mapred.LocalJobRunner: Starting task: attempt_local384800455_0001_r_000000_0\n","2024-05-06 15:36:10,002 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:10,002 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:10,002 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:10,010 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@18a0f39f\n","2024-05-06 15:36:10,014 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:10,039 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:36:10,054 INFO reduce.EventFetcher: attempt_local384800455_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:36:10,092 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local384800455_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n","2024-05-06 15:36:10,096 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local384800455_0001_m_000000_0\n","2024-05-06 15:36:10,099 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n","2024-05-06 15:36:10,102 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:36:10,104 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:10,104 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:36:10,113 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:10,113 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-05-06 15:36:10,116 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:36:10,116 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n","2024-05-06 15:36:10,117 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:36:10,117 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:10,118 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-05-06 15:36:10,119 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:10,126 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n","2024-05-06 15:36:10,130 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-05-06 15:36:10,133 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-05-06 15:36:10,151 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:10,153 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:10,154 INFO streaming.PipeMapRed: Records R/W=9/1\n","2024-05-06 15:36:10,155 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:10,156 INFO mapred.Task: Task:attempt_local384800455_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:10,157 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:10,157 INFO mapred.Task: Task attempt_local384800455_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:36:10,158 INFO output.FileOutputCommitter: Saved output of task 'attempt_local384800455_0001_r_000000_0' to file:/content/wordcount/output\n","2024-05-06 15:36:10,162 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n","2024-05-06 15:36:10,164 INFO mapred.Task: Task 'attempt_local384800455_0001_r_000000_0' done.\n","2024-05-06 15:36:10,165 INFO mapred.Task: Final Counters for attempt_local384800455_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1429\n","\t\tFILE: Number of bytes written=716724\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=5\n","\t\tSpilled Records=9\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=313524224\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=68\n","2024-05-06 15:36:10,165 INFO mapred.LocalJobRunner: Finishing task: attempt_local384800455_0001_r_000000_0\n","2024-05-06 15:36:10,165 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:36:10,652 INFO mapreduce.Job: Job job_local384800455_0001 running in uber mode : false\n","2024-05-06 15:36:10,653 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:36:10,654 INFO mapreduce.Job: Job job_local384800455_0001 completed successfully\n","2024-05-06 15:36:10,666 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=2622\n","\t\tFILE: Number of bytes written=1433278\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=5\n","\t\tSpilled Records=18\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=11\n","\t\tTotal committed heap usage (bytes)=627048448\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","\tFile Output Format Counters \n","\t\tBytes Written=68\n","2024-05-06 15:36:10,666 INFO streaming.StreamJob: Output directory: wordcount/output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/output 2>/dev/null\n","mapred streaming \\\n","  -file map.sh \\\n","  -file reduce.sh \\\n","  -input wordcount/input \\\n","  -output wordcount/output \\\n","  -mapper map.sh \\\n","  -reducer reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"1tcDo4ZYcZ9k"},"source":["Let's check the output on the HDFS filesystem"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5NTHrTWOcZ9k","outputId":"4d6b8d41-3327-46f4-8b21-d8d3c743b182","executionInfo":{"status":"ok","timestamp":1715009773613,"user_tz":-420,"elapsed":1736,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["apple \t 2\n","banana \t 1\n","orange \t 1\n","peach \t 4\n","pineapple \t 1\n"]}],"source":["!hdfs dfs -cat wordcount/output/part*|head"]},{"cell_type":"markdown","metadata":{"id":"IHjgDA3bcZ9k"},"source":["## Run a mapreduce job with more data <a name=\"moredata\"></a>\n","\n","Let's create a datafile by downloading some real data, for instance from a Web page. This example will be used to introduce some advanced configurations.\n","\n","Next, we download a URL with `wget` and filter out HTML tags with a `sed` regular expression."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"KBvaOLI3cZ9k","executionInfo":{"status":"ok","timestamp":1715009773614,"user_tz":-420,"elapsed":6,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[],"source":["%%bash\n","URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n","wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OCipedVcZ9k","outputId":"36be8149-5b75-4827-b6d1-8a6d175f1886","executionInfo":{"status":"ok","timestamp":1715009774230,"user_tz":-420,"elapsed":621,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\r\t1\n","\r\t1\n","\r\t1\n","\r\t1\n","window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r\t1\n","\r\t1\n","\r\t1\n","\r\t1\n","Und\t1\n","wo\t1\n"]}],"source":["!cat sample_article.txt|./map.sh|head"]},{"cell_type":"markdown","metadata":{"id":"toPKplxhcZ9l"},"source":["As usual, with real data there's some more work to do. Here we see that the mapper script doesn't skip empty lines. Let's modify it so that empty lines are skipped."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UaEGTMLfcZ9l","outputId":"a4018856-8643-4743-916f-52d6b1870e3d","executionInfo":{"status":"ok","timestamp":1715009774230,"user_tz":-420,"elapsed":11,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting map.sh\n"]}],"source":["%%writefile map.sh\n","#!/bin/bash\n","\n","while read line\n","do\n"," for word in $line\n"," do\n","  if [[ \"$line\" =~ [^[:space:]] ]]\n","  then\n","    if [ -n \"$word\" ]\n","    then\n","    echo -e ${word} \"\\t1\"\n","    fi\n","  fi\n"," done\n","done"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyY9YiUdcZ9l","outputId":"abeac942-3600-43f0-d5c1-9ffc9d090aa9","executionInfo":{"status":"ok","timestamp":1715009774231,"user_tz":-420,"elapsed":9,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t1\n","Und \t1\n","wo \t1\n","warst \t1\n","du \t1\n","beim \t1\n","Fall \t1\n","der \t1\n","Mauer? \t1\n","- \t1\n"]}],"source":["!cat sample_article.txt|./map.sh|head"]},{"cell_type":"markdown","metadata":{"id":"99aslnFtcZ9l"},"source":["Now the output of `map.sh` looks better!\n","\n","<b>Note:</b> when working with real data we need in general some more preprocessing in order to remove control characters or invalid unicode.\n","\n","Time to run MapReduce again with the new data, but first we need to \"put\" the data on HDFS."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxM4UH92cZ9l","outputId":"8fe0e8e0-13f8-4a96-d622-d24f56b8ae47","executionInfo":{"status":"ok","timestamp":1715009778011,"user_tz":-420,"elapsed":3786,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/input\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/input 2>/dev/null\n","hdfs dfs -put sample_article.txt wordcount/input"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yE8b7SWdcZ9l","outputId":"5ffd146a-2cd3-41a1-dee6-b8b5bc899367","executionInfo":{"status":"ok","timestamp":1715009779379,"user_tz":-420,"elapsed":1374,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r--   1 root root     29.0 K 2024-05-06 15:36 wordcount/input\n"]}],"source":["# check that the folder wordcount/input on HDFS only contains sample_article.txt\n","!hdfs dfs -ls -h wordcount/input"]},{"cell_type":"markdown","metadata":{"id":"uiV_tcHLcZ9m"},"source":["Check the reducer"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jg5z8JmVcZ9m","outputId":"b9ff4307-26aa-4b5a-f3d9-987d6482f4e2","executionInfo":{"status":"ok","timestamp":1715009779893,"user_tz":-420,"elapsed":519,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t 1\n","Und \t 1\n","wo \t 1\n","warst \t 1\n","du \t 1\n","beim \t 1\n","Fall \t 1\n","der \t 1\n","Mauer? \t 1\n","- \t 1\n"]}],"source":["!cat sample_article.txt|./map.sh|./reduce.sh|head"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LE8j6SbcZ9m","outputId":"939f277a-b42a-4457-dfef-ad53635cb7a2","executionInfo":{"status":"ok","timestamp":1715009785035,"user_tz":-420,"elapsed":5147,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/output\n","packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob5804039595091606756.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:36:20,977 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-05-06 15:36:21,740 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:36:21,883 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:36:21,883 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:36:21,902 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:22,067 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:36:22,086 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:36:22,358 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1799467973_0001\n","2024-05-06 15:36:22,359 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:36:22,657 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local1799467973_0001_721896dc-c72f-443b-9028-1aafd8232f57/map.sh\n","2024-05-06 15:36:22,684 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local1799467973_0001_93894ad1-2eba-4ee6-aa9e-9b50e039e0a8/reduce.sh\n","2024-05-06 15:36:22,788 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:36:22,789 INFO mapreduce.Job: Running job: job_local1799467973_0001\n","2024-05-06 15:36:22,795 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:36:22,797 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:36:22,808 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:22,808 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:22,885 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:36:22,892 INFO mapred.LocalJobRunner: Starting task: attempt_local1799467973_0001_m_000000_0\n","2024-05-06 15:36:22,948 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:22,948 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:22,979 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:22,996 INFO mapred.MapTask: Processing split: file:/content/wordcount/input:0+29720\n","2024-05-06 15:36:23,013 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:36:23,059 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:36:23,059 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:36:23,059 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:36:23,059 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:36:23,059 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:36:23,063 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:36:23,074 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n","2024-05-06 15:36:23,082 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:36:23,083 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:36:23,083 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:36:23,084 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:36:23,084 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:36:23,085 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:36:23,086 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:36:23,086 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:36:23,087 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:36:23,087 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:36:23,088 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:36:23,088 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:36:23,104 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,104 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,105 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,114 INFO streaming.PipeMapRed: Records R/W=186/1\n","2024-05-06 15:36:23,351 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:23,352 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:23,358 INFO mapred.LocalJobRunner: \n","2024-05-06 15:36:23,359 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:36:23,359 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:36:23,359 INFO mapred.MapTask: bufstart = 0; bufend = 32527; bufvoid = 104857600\n","2024-05-06 15:36:23,359 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209816(104839264); length = 4581/6553600\n","2024-05-06 15:36:23,394 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:36:23,414 INFO mapred.Task: Task:attempt_local1799467973_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:23,418 INFO mapred.LocalJobRunner: Records R/W=186/1\n","2024-05-06 15:36:23,419 INFO mapred.Task: Task 'attempt_local1799467973_0001_m_000000_0' done.\n","2024-05-06 15:36:23,448 INFO mapred.Task: Final Counters for attempt_local1799467973_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=31159\n","\t\tFILE: Number of bytes written=754873\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=186\n","\t\tMap output records=1146\n","\t\tMap output bytes=32527\n","\t\tMap output materialized bytes=34873\n","\t\tInput split bytes=81\n","\t\tCombine input records=0\n","\t\tSpilled Records=1146\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=361758720\n","\tFile Input Format Counters \n","\t\tBytes Read=29968\n","2024-05-06 15:36:23,448 INFO mapred.LocalJobRunner: Finishing task: attempt_local1799467973_0001_m_000000_0\n","2024-05-06 15:36:23,449 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:36:23,460 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:36:23,462 INFO mapred.LocalJobRunner: Starting task: attempt_local1799467973_0001_r_000000_0\n","2024-05-06 15:36:23,512 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:23,512 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:23,512 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:23,517 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@722c49b4\n","2024-05-06 15:36:23,519 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:23,537 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:36:23,544 INFO reduce.EventFetcher: attempt_local1799467973_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:36:23,577 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1799467973_0001_m_000000_0 decomp: 34869 len: 34873 to MEMORY\n","2024-05-06 15:36:23,581 INFO reduce.InMemoryMapOutput: Read 34869 bytes from map-output for attempt_local1799467973_0001_m_000000_0\n","2024-05-06 15:36:23,585 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34869, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34869\n","2024-05-06 15:36:23,589 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:36:23,591 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:23,591 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:36:23,600 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:23,600 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n","2024-05-06 15:36:23,608 INFO reduce.MergeManagerImpl: Merged 1 segments, 34869 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:36:23,609 INFO reduce.MergeManagerImpl: Merging 1 files, 34873 bytes from disk\n","2024-05-06 15:36:23,618 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:36:23,622 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:23,624 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n","2024-05-06 15:36:23,625 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:23,636 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n","2024-05-06 15:36:23,640 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-05-06 15:36:23,642 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-05-06 15:36:23,658 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,659 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,661 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,669 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:23,676 INFO streaming.PipeMapRed: Records R/W=1146/1\n","2024-05-06 15:36:23,735 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:23,736 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:23,738 INFO mapred.Task: Task:attempt_local1799467973_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:23,740 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:23,740 INFO mapred.Task: Task attempt_local1799467973_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:36:23,742 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1799467973_0001_r_000000_0' to file:/content/wordcount/output\n","2024-05-06 15:36:23,743 INFO mapred.LocalJobRunner: Records R/W=1146/1 > reduce\n","2024-05-06 15:36:23,745 INFO mapred.Task: Task 'attempt_local1799467973_0001_r_000000_0' done.\n","2024-05-06 15:36:23,745 INFO mapred.Task: Final Counters for attempt_local1799467973_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=100937\n","\t\tFILE: Number of bytes written=819338\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=736\n","\t\tReduce shuffle bytes=34873\n","\t\tReduce input records=1146\n","\t\tReduce output records=746\n","\t\tSpilled Records=1146\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=361758720\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=29592\n","2024-05-06 15:36:23,746 INFO mapred.LocalJobRunner: Finishing task: attempt_local1799467973_0001_r_000000_0\n","2024-05-06 15:36:23,746 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:36:23,795 INFO mapreduce.Job: Job job_local1799467973_0001 running in uber mode : false\n","2024-05-06 15:36:23,797 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:36:23,801 INFO mapreduce.Job: Job job_local1799467973_0001 completed successfully\n","2024-05-06 15:36:23,820 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=132096\n","\t\tFILE: Number of bytes written=1574211\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=186\n","\t\tMap output records=1146\n","\t\tMap output bytes=32527\n","\t\tMap output materialized bytes=34873\n","\t\tInput split bytes=81\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=736\n","\t\tReduce shuffle bytes=34873\n","\t\tReduce input records=1146\n","\t\tReduce output records=746\n","\t\tSpilled Records=2292\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=723517440\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29968\n","\tFile Output Format Counters \n","\t\tBytes Written=29592\n","2024-05-06 15:36:23,820 INFO streaming.StreamJob: Output directory: wordcount/output\n"]}],"source":["%%bash\n","hadoop fs -rmr wordcount/output 2>/dev/null\n","mapred streaming \\\n","  -file map.sh \\\n","  -file reduce.sh \\\n","  -input wordcount/input \\\n","  -output wordcount/output \\\n","  -mapper map.sh \\\n","  -reducer reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"l4pWfq08cZ9m"},"source":["Check the output on HDFS"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klQOTweGcZ9m","outputId":"c5859f00-a26c-433f-ebf4-b1c0ebe466ed","executionInfo":{"status":"ok","timestamp":1715009787699,"user_tz":-420,"elapsed":2666,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-05-06 15:36 wordcount/output/_SUCCESS\n","-rw-r--r--   1 root root      29352 2024-05-06 15:36 wordcount/output/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output"]},{"cell_type":"markdown","metadata":{"id":"Gcv-5xVBcZ9m"},"source":["This job took a few seconds and this is quite some time for such a small file (4KB). This is due to the overhead of distributing the data and running the Hadoop framework.\n","The advantage of Hadoop can be appreciated only for large datasets."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdMLkvJncZ9m","outputId":"10d8519f-894b-46d4-ddfe-0c2399674060","executionInfo":{"status":"ok","timestamp":1715009788867,"user_tz":-420,"elapsed":1170,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["!!n.frames[t]; \t 1\n","!0)); \t 1\n","!1)) \t 1\n","!1, \t 1\n","!= \t 1\n","!== \t 2\n","!function \t 2\n","!r \t 1\n","\"'+n+'\"',o)}return{key:r,value:e.substr(t+1)}},t._renewCache=function(){t._cache=t._getCacheFromString(t._document.cookie),t._cachedDocumentCookie=t._document.cookie},t._areEnabled=function(){var \t 1\n","\"))}function \t 1\n","cat: Unable to write to output stream.\n"]}],"source":["!hdfs dfs -cat wordcount/output/part-00000|head"]},{"cell_type":"markdown","metadata":{"id":"Jox3UwescZ9n"},"source":["### Sort the output with `sort` <a name=\"sortoutput\"></a>\n","\n","We've obtained a list of tokens that appear in the file followed by their frequencies.\n","\n","The output of the reducer is sorted by key (the word) because that's the ordering that the reducer becomes from the mapper. If we're interested in sorting the data by frequency, we can use the Unix `sort` command (with the options `k2`, `n`, `r` respectively \"by field 2\", \"numeric\", \"reverse\")."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knu9lMvEcZ9n","outputId":"53ff9b73-a189-4aa0-a830-fd592d759974","executionInfo":{"status":"ok","timestamp":1715009790401,"user_tz":-420,"elapsed":1537,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["= \t 40\n","{ \t 22\n","var \t 22\n","&& \t 19\n","strict\";function \t 13\n","} \t 12\n","in \t 12\n","not \t 12\n","to \t 10\n","e&&e.__esModule?e:{\"default\":e}}function \t 9\n"]}],"source":["!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"]},{"cell_type":"markdown","metadata":{"id":"usGiyUlOcZ9n"},"source":["The most common word appears to be \"die\" (the German for the definite article \"the\")."]},{"cell_type":"markdown","metadata":{"id":"c1f2mtsLcZ9n"},"source":["### Sort the output with another MapReduce job <a name=\"sortoutputMR\"></a>\n","\n","If we wanted to sort the output of the reducer using the mapreduce framework, we could employ a simple trick: create a mapper that interchanges words with their frequency values. Since by construction mappers sort their output by key, we get the desired sorting as a side-effect.\n","\n","Call the new mapper `swap_keyval.sh`."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dSWH46vcZ9n","outputId":"62116bf5-f04b-484d-9adb-028b66f289be","executionInfo":{"status":"ok","timestamp":1715009790682,"user_tz":-420,"elapsed":282,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing swap_keyval.sh\n"]}],"source":["%%writefile swap_keyval.sh\n","#!/bin/bash\n","# This script will read one line at a time and swap key/value\n","# For instance, the line \"word 100\" will become \"100 word\"\n","\n","while read key val\n","do\n"," printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"VvCo2Y-VcZ9n"},"source":["We are going to run the swap mapper script on the output of the previous mapreduce job. Note that in the below cell we are not deleting the previous output but instead we're saving the output from the current job in a new folder `output_sorted`.\n","\n","Nice thing about running a job on the output of a preceding job is that we do not need to upload files to HDFS because the data is already on HDFS. Not so nice: writing data to disk at each step of a data transformation pipeline takes time and this can be costly for longer data pipelines. This is one of the shortcomings of MapReduce that are addressed by [Apache Spark](https://spark.apache.org/)."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrPCekO2cZ9n","outputId":"d176fc01-5d1a-4fd3-fd6b-801f3ca989a8","executionInfo":{"status":"ok","timestamp":1715009796880,"user_tz":-420,"elapsed":6200,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["packageJobJar: [swap_keyval.sh] [] /tmp/streamjob13101516517000106172.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:36:32,242 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-05-06 15:36:33,001 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:36:33,158 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:36:33,158 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:36:33,183 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:33,433 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:36:33,480 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:36:33,971 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local301417426_0001\n","2024-05-06 15:36:33,971 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:36:34,332 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local301417426_0001_778386e8-fd0f-4599-ad53-b1ee90ab9afb/swap_keyval.sh\n","2024-05-06 15:36:34,482 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:36:34,484 INFO mapreduce.Job: Running job: job_local301417426_0001\n","2024-05-06 15:36:34,491 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:36:34,498 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:36:34,508 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:34,508 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:34,587 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:36:34,593 INFO mapred.LocalJobRunner: Starting task: attempt_local301417426_0001_m_000000_0\n","2024-05-06 15:36:34,669 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:34,670 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:34,704 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:34,719 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n","2024-05-06 15:36:34,740 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:36:34,790 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:36:34,790 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:36:34,790 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:36:34,790 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:36:34,790 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:36:34,795 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:36:34,811 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n","2024-05-06 15:36:34,820 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:36:34,822 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:36:34,823 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:36:34,823 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:36:34,824 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:36:34,824 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:36:34,826 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:36:34,827 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:36:34,827 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:36:34,828 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:36:34,829 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:36:34,830 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:36:34,861 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:34,862 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:34,863 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:34,884 INFO streaming.PipeMapRed: Records R/W=746/1\n","2024-05-06 15:36:34,905 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:34,908 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:34,912 INFO mapred.LocalJobRunner: \n","2024-05-06 15:36:34,912 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:36:34,912 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:36:34,912 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n","2024-05-06 15:36:34,912 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n","2024-05-06 15:36:34,929 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:36:34,943 INFO mapred.Task: Task:attempt_local301417426_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:34,945 INFO mapred.LocalJobRunner: Records R/W=746/1\n","2024-05-06 15:36:34,945 INFO mapred.Task: Task 'attempt_local301417426_0001_m_000000_0' done.\n","2024-05-06 15:36:34,966 INFO mapred.Task: Final Counters for attempt_local301417426_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=30232\n","\t\tFILE: Number of bytes written=743546\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tSpilled Records=746\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=408944640\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","2024-05-06 15:36:34,967 INFO mapred.LocalJobRunner: Finishing task: attempt_local301417426_0001_m_000000_0\n","2024-05-06 15:36:34,967 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:36:34,975 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:36:34,977 INFO mapred.LocalJobRunner: Starting task: attempt_local301417426_0001_r_000000_0\n","2024-05-06 15:36:34,995 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:34,995 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:34,996 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:35,000 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@722c49b4\n","2024-05-06 15:36:35,006 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:35,026 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:36:35,032 INFO reduce.EventFetcher: attempt_local301417426_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:36:35,066 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local301417426_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n","2024-05-06 15:36:35,072 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local301417426_0001_m_000000_0\n","2024-05-06 15:36:35,074 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n","2024-05-06 15:36:35,076 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:36:35,078 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:35,078 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:36:35,086 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:35,086 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n","2024-05-06 15:36:35,096 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:36:35,097 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n","2024-05-06 15:36:35,101 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:36:35,101 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:35,102 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n","2024-05-06 15:36:35,103 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:35,134 INFO mapred.Task: Task:attempt_local301417426_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:35,135 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:35,135 INFO mapred.Task: Task attempt_local301417426_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:36:35,138 INFO output.FileOutputCommitter: Saved output of task 'attempt_local301417426_0001_r_000000_0' to file:/content/wordcount/output2\n","2024-05-06 15:36:35,139 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-05-06 15:36:35,139 INFO mapred.Task: Task 'attempt_local301417426_0001_r_000000_0' done.\n","2024-05-06 15:36:35,140 INFO mapred.Task: Final Counters for attempt_local301417426_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=89170\n","\t\tFILE: Number of bytes written=801087\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=746\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=408944640\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-05-06 15:36:35,140 INFO mapred.LocalJobRunner: Finishing task: attempt_local301417426_0001_r_000000_0\n","2024-05-06 15:36:35,140 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:36:35,489 INFO mapreduce.Job: Job job_local301417426_0001 running in uber mode : false\n","2024-05-06 15:36:35,491 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:36:35,494 INFO mapreduce.Job: Job job_local301417426_0001 completed successfully\n","2024-05-06 15:36:35,507 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=119402\n","\t\tFILE: Number of bytes written=1544633\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=1492\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=817889280\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-05-06 15:36:35,508 INFO streaming.StreamJob: Output directory: wordcount/output2\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/output2 2>/dev/null\n","mapred streaming \\\n","  -file swap_keyval.sh \\\n","  -input wordcount/output \\\n","  -output wordcount/output2 \\\n","  -mapper swap_keyval.sh"]},{"cell_type":"markdown","metadata":{"id":"C4ubxMCQcZ9o"},"source":["Check the output on HDFS"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYI62M7bcZ9o","outputId":"4adbe0f5-6388-4722-aa14-7d87137ec3c0","executionInfo":{"status":"ok","timestamp":1715009799308,"user_tz":-420,"elapsed":2443,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-05-06 15:36 wordcount/output2/_SUCCESS\n","-rw-r--r--   1 root root      27860 2024-05-06 15:36 wordcount/output2/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output2"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWD3YrfKcZ9o","outputId":"4abae58b-8341-485f-9cf8-844fdee7238f","executionInfo":{"status":"ok","timestamp":1715009801152,"user_tz":-420,"elapsed":1849,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1\t!!n.frames[t];\n","1\tüberraschen.\n","1\tüber\n","1\t©\n","1\t},\n","1\t}();\n","1\t}(),\n","1\t{};\n","1\ty(){E[\"default\"].debug(\"User\n","1\ty(),j(),void(ne=D());case\n","cat: Unable to write to output stream.\n"]}],"source":["!hdfs dfs -cat wordcount/output2/part-00000|head"]},{"cell_type":"markdown","metadata":{"id":"sWwfJXkIcZ9o"},"source":["Mapper uses by default ascending order to sort by key. We could have changed that with an option but for now let's look at the end of the file."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRgSwZNPcZ9o","outputId":"b755ed42-85bf-4073-a184-1f41da1a3079","executionInfo":{"status":"ok","timestamp":1715009802893,"user_tz":-420,"elapsed":1746,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["7\t==\n","7\t:\n","7\tdie\n","7\t?\n","7\tif\n","7\ttypeof\n","8\t0\n","8\tn(e){return\n","9\te&&e.__esModule?e:{\"default\":e}}function\n","9\tr\n"]}],"source":["!hdfs dfs -cat wordcount/output2/part-00000|tail"]},{"cell_type":"markdown","metadata":{"id":"wR-szt7tcZ9o"},"source":["### Configure sort with `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n","\n","In general, we can determine how mappers are going to sort their output by configuring the comparator directive to use the special class [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n","<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n","    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n","    \n","This class has some options similar to the Unix `sort`(`-n` to sort numerically, `-r` for reverse sorting, `-k pos1[,pos2]` for specifying fields to sort by).\n","\n","Let us see the comparator in action on our data to get the desired result. Note that this time we are removing `output2` because we're running the second mapreduce job again."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQEC16YPcZ9o","outputId":"5d3f3b31-7ea6-4505-85a6-e0645165b4fc","executionInfo":{"status":"ok","timestamp":1715009808245,"user_tz":-420,"elapsed":5356,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/output2\n","packageJobJar: [swap_keyval.sh] [] /tmp/streamjob13979993348862742983.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-05-06 15:36:44,001 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-05-06 15:36:44,802 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-05-06 15:36:44,946 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-05-06 15:36:44,947 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-05-06 15:36:44,976 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:45,184 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-05-06 15:36:45,204 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-05-06 15:36:45,454 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1275610144_0001\n","2024-05-06 15:36:45,454 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-05-06 15:36:45,783 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local1275610144_0001_68f3e980-bb86-4b6c-acba-967f3bd88583/swap_keyval.sh\n","2024-05-06 15:36:45,888 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-05-06 15:36:45,889 INFO mapreduce.Job: Running job: job_local1275610144_0001\n","2024-05-06 15:36:45,894 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-05-06 15:36:45,896 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-05-06 15:36:45,902 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:45,902 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:45,942 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-05-06 15:36:45,946 INFO mapred.LocalJobRunner: Starting task: attempt_local1275610144_0001_m_000000_0\n","2024-05-06 15:36:45,994 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:45,995 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:46,024 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:46,039 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n","2024-05-06 15:36:46,057 INFO mapred.MapTask: numReduceTasks: 1\n","2024-05-06 15:36:46,100 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-05-06 15:36:46,100 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-05-06 15:36:46,100 INFO mapred.MapTask: soft limit at 83886080\n","2024-05-06 15:36:46,100 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-05-06 15:36:46,100 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-05-06 15:36:46,105 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-05-06 15:36:46,114 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n","2024-05-06 15:36:46,127 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-05-06 15:36:46,133 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-05-06 15:36:46,133 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-05-06 15:36:46,133 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-05-06 15:36:46,134 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-05-06 15:36:46,134 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-05-06 15:36:46,138 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-05-06 15:36:46,138 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-05-06 15:36:46,138 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-05-06 15:36:46,138 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-05-06 15:36:46,139 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-05-06 15:36:46,140 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-05-06 15:36:46,158 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:46,158 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:46,159 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-05-06 15:36:46,176 INFO streaming.PipeMapRed: Records R/W=746/1\n","2024-05-06 15:36:46,197 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-05-06 15:36:46,201 INFO streaming.PipeMapRed: mapRedFinished\n","2024-05-06 15:36:46,204 INFO mapred.LocalJobRunner: \n","2024-05-06 15:36:46,204 INFO mapred.MapTask: Starting flush of map output\n","2024-05-06 15:36:46,204 INFO mapred.MapTask: Spilling map output\n","2024-05-06 15:36:46,205 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n","2024-05-06 15:36:46,205 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n","2024-05-06 15:36:46,224 INFO mapred.MapTask: Finished spill 0\n","2024-05-06 15:36:46,239 INFO mapred.Task: Task:attempt_local1275610144_0001_m_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:46,241 INFO mapred.LocalJobRunner: Records R/W=746/1\n","2024-05-06 15:36:46,246 INFO mapred.Task: Task 'attempt_local1275610144_0001_m_000000_0' done.\n","2024-05-06 15:36:46,256 INFO mapred.Task: Final Counters for attempt_local1275610144_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=30232\n","\t\tFILE: Number of bytes written=747919\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tSpilled Records=746\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","2024-05-06 15:36:46,256 INFO mapred.LocalJobRunner: Finishing task: attempt_local1275610144_0001_m_000000_0\n","2024-05-06 15:36:46,256 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-05-06 15:36:46,260 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-05-06 15:36:46,260 INFO mapred.LocalJobRunner: Starting task: attempt_local1275610144_0001_r_000000_0\n","2024-05-06 15:36:46,298 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-05-06 15:36:46,298 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-05-06 15:36:46,298 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-05-06 15:36:46,304 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@14929099\n","2024-05-06 15:36:46,306 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-05-06 15:36:46,326 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-05-06 15:36:46,329 INFO reduce.EventFetcher: attempt_local1275610144_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-05-06 15:36:46,361 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1275610144_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n","2024-05-06 15:36:46,366 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local1275610144_0001_m_000000_0\n","2024-05-06 15:36:46,368 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n","2024-05-06 15:36:46,370 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-05-06 15:36:46,371 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:46,371 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-05-06 15:36:46,376 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:46,376 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n","2024-05-06 15:36:46,383 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n","2024-05-06 15:36:46,383 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n","2024-05-06 15:36:46,384 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-05-06 15:36:46,384 INFO mapred.Merger: Merging 1 sorted segments\n","2024-05-06 15:36:46,385 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n","2024-05-06 15:36:46,385 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:46,410 INFO mapred.Task: Task:attempt_local1275610144_0001_r_000000_0 is done. And is in the process of committing\n","2024-05-06 15:36:46,411 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-05-06 15:36:46,411 INFO mapred.Task: Task attempt_local1275610144_0001_r_000000_0 is allowed to commit now\n","2024-05-06 15:36:46,412 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1275610144_0001_r_000000_0' to file:/content/wordcount/output2\n","2024-05-06 15:36:46,414 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-05-06 15:36:46,414 INFO mapred.Task: Task 'attempt_local1275610144_0001_r_000000_0' done.\n","2024-05-06 15:36:46,415 INFO mapred.Task: Final Counters for attempt_local1275610144_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=89170\n","\t\tFILE: Number of bytes written=805460\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=746\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-05-06 15:36:46,415 INFO mapred.LocalJobRunner: Finishing task: attempt_local1275610144_0001_r_000000_0\n","2024-05-06 15:36:46,415 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-05-06 15:36:46,893 INFO mapreduce.Job: Job job_local1275610144_0001 running in uber mode : false\n","2024-05-06 15:36:46,895 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-05-06 15:36:46,896 INFO mapreduce.Job: Job job_local1275610144_0001 completed successfully\n","2024-05-06 15:36:46,905 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=119402\n","\t\tFILE: Number of bytes written=1553379\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=1492\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=864026624\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-05-06 15:36:46,905 INFO streaming.StreamJob: Output directory: wordcount/output2\n"]}],"source":["%%bash\n","hdfs dfs -rmr wordcount/output2 2>/dev/null\n","comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n","mapred streaming \\\n","  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n","  -D mapreduce.partition.keycomparator.options=-nr \\\n","  -file swap_keyval.sh \\\n","  -input wordcount/output \\\n","  -output wordcount/output2 \\\n","  -mapper swap_keyval.sh"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-wncVDicZ9p","outputId":"5bb2b643-40d8-4bcc-9555-35001b0fc4d7","executionInfo":{"status":"ok","timestamp":1715009810580,"user_tz":-420,"elapsed":2348,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-05-06 15:36 wordcount/output2/_SUCCESS\n","-rw-r--r--   1 root root      27860 2024-05-06 15:36 wordcount/output2/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output2"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q6lP03D1cZ9p","outputId":"3646cf52-5282-443e-8fff-057490e03883","executionInfo":{"status":"ok","timestamp":1715009812417,"user_tz":-420,"elapsed":1841,"user":{"displayName":"5886 Đặng Văn Khải","userId":"00827885371675823732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["40\t=\n","22\t{\n","22\tvar\n","19\t&&\n","13\tstrict\";function\n","12\tnot\n","12\t}\n","12\tin\n","10\tto\n","9\te&&e.__esModule?e:{\"default\":e}}function\n","cat: Unable to write to output stream.\n"]}],"source":["!hdfs dfs -cat wordcount/output2/part-00000|head"]},{"cell_type":"markdown","metadata":{"id":"3MJU6Tq9cZ9p"},"source":["Now we get the output in the desired order."]},{"cell_type":"markdown","metadata":{"id":"jGm7kQblcZ9p"},"source":["### Specifying Configuration Variables with the -D Option <a name=\"configuration_variables\"></a>\n","\n","With the `-D` option it is possible to override options set in the default configuration file [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n","(see the [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n","\n","One option that might come handy when dealing with out-of-memory issues in the sorting phase is the size in MB of the memory reserved for sorting `mapreduce.task.io.sort.mb`:\n"," <html>\n","    <pre>-D mapreduce.task.io.sort.mb=512\n","    </pre>\n"," </html>\n","\n"," **Note:** the maximum value for `mapreduce.task.io.sort.mb` is 2047.   "]},{"cell_type":"markdown","metadata":{"id":"rovwCXRMcZ9p"},"source":["## What is word count useful for? <a name=\"wordcount\"></a>\n","Counting the frequencies of words is at the basis of _indexing_ and it facilitates the retrieval of relevant documents in search engines."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}